services:
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    environment:
      - MODEL_PATH=/app/models/deepseek-llm-7b-chat.Q6_K.gguf
      - THREADS=6  # Adjust based on your CPU
    restart: unless-stopped